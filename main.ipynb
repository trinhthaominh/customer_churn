{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex2: Logistic Regression Consulting Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Binary Customer Churn\n",
    "\n",
    "A marketing agency has many customers that use their service to produce ads for the client/customer websites. They've noticed that they have quite a bit of churn in clients. They basically randomly assign account managers right now, but want you to create a machine learning model that will help predict which customers will churn (stop buying their service) so that they can correctly assign the customers most at risk to churn an account manager. Luckily they have some historical data, can you help them out? Create a classification algorithm that will help classify whether or not a customer churned. Then the company can test this against incoming data for future customers to predict which customers will churn and assign them an account manager.\n",
    "\n",
    "The data is saved as customer_churn.csv. Here are the fields and their definitions:\n",
    "\n",
    "    Name : Name of the latest contact at Company\n",
    "    Age: Customer Age\n",
    "    Total_Purchase: Total Ads Purchased\n",
    "    Account_Manager: Binary 0=No manager, 1= Account manager assigned\n",
    "    Years: Totaly Years as a customer\n",
    "    Num_sites: Number of websites that use the service.\n",
    "    Onboard_date: Date that the name of the latest contact was onboarded\n",
    "    Location: Client HQ Address\n",
    "    Company: Name of Client Company\n",
    "    \n",
    "Once you've created the model and evaluated it, test out the model on some new data (you can think of this almost like a hold-out set) that your client has provided, saved under new_customers.csv. The client wants to know which customers are most likely to churn given this data (they don't have the label yet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/khoatrinh/Spark/spark-3.5.1-bin-hadoop3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 15:34:09 WARN Utils: Your hostname, macbook-cua-Minh.local resolves to a loopback address: 127.0.0.1; using 172.18.180.89 instead (on interface en0)\n",
      "24/03/17 15:34:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/17 15:34:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.18.180.89:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc =SparkContext()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.18.180.89:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x109428400>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession(sc)\n",
    "spark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.ml.feature import Binarizer, Bucketizer, OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel \n",
    "from pyspark.ml.regression import LinearRegression, LinearRegressionModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
      "|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|             Company|Churn|\n",
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
      "|Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:40|10265 Elizabeth M...|          Harvey LLC|    1|\n",
      "|   Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:46|6157 Frank Garden...|          Wilson PLC|    1|\n",
      "|     Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|2016-06-29 06:20:07|1331 Keith Court ...|Miller, Johnson a...|    1|\n",
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_churn = spark.read.csv('customer_churn.csv', header = True, inferSchema = True)\n",
    "customer_churn.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_churn.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_churn.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_churn = customer_churn.dropna()\n",
    "customer_churn.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 15:34:14 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+---------------------------------------------------+-------------------------+-------------------+\n",
      "|summary|Names        |Age              |Total_Purchase   |Account_Manager   |Years            |Num_Sites         |Location                                           |Company                  |Churn              |\n",
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+---------------------------------------------------+-------------------------+-------------------+\n",
      "|count  |900          |900              |900              |900               |900              |900               |900                                                |900                      |900                |\n",
      "|mean   |NULL         |41.81666666666667|10062.82403333334|0.4811111111111111|5.27315555555555 |8.587777777777777 |NULL                                               |NULL                     |0.16666666666666666|\n",
      "|stddev |NULL         |6.127560416916251|2408.644531858096|0.4999208935073339|1.274449013194616|1.7648355920350969|NULL                                               |NULL                     |0.3728852122772358 |\n",
      "|min    |Aaron King   |22.0             |100.0            |0                 |1.0              |3.0               |00103 Jeffrey Crest Apt. 205 Padillaville, IA 90755|Abbott-Thompson          |0                  |\n",
      "|max    |Zachary Walsh|65.0             |18026.01         |1                 |9.15             |14.0              |Unit 9800 Box 2878 DPO AA 75157                    |Zuniga, Clark and Shaffer|1                  |\n",
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+---------------------------------------------------+-------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_churn.describe().show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_churn.createOrReplaceTempView('customer_churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DISTINCT Location)|\n",
      "+------------------------+\n",
      "|                     900|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''SELECT COUNT(DISTINCT Location) FROM customer_churn'''\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT Company)|\n",
      "+-----------------------+\n",
      "|                    873|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''SELECT COUNT(DISTINCT Company) FROM customer_churn'''\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+---------------+-----+---------+-------------------+--------------------+-----+\n",
      "| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|             Company|Churn|\n",
      "+----+--------------+---------------+-----+---------+-------------------+--------------------+-----+\n",
      "|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:40|          Harvey LLC|    1|\n",
      "|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:46|          Wilson PLC|    1|\n",
      "|38.0|      12884.75|              0| 6.67|     12.0|2016-06-29 06:20:07|Miller, Johnson a...|    1|\n",
      "+----+--------------+---------------+-----+---------+-------------------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = customer_churn.drop(*['Names', 'Location'])\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('log_Total_Purchase', log(col('Total_Purchase')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  9.181747690263084\n",
      "Stddev:  0.2986488999263224\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean and std for filtering outliers\n",
    "mean_val = df.agg({'log_Total_Purchase': \"mean\"}).collect()[0][0]\n",
    "stddev_val = df.agg({'log_Total_Purchase': \"stddev\"}).collect()[0][0]\n",
    "\n",
    "print('Mean: ', mean_val)\n",
    "print('Stddev: ', stddev_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lo = mean_val - 3 * stddev_val\n",
    "hi = mean_val + 3 * stddev_val\n",
    "# Filtering df_sub without outliers\n",
    "df_sub = df.where((df['log_Total_Purchase'] > lo) & (df['log_Total_Purchase'] < hi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------------------+------------------+------------------+--------------------+-------------------+-------------------+\n",
      "|summary|               Age|    Total_Purchase|    Account_Manager|             Years|         Num_Sites|             Company|              Churn| log_Total_Purchase|\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+--------------------+-------------------+-------------------+\n",
      "|  count|               895|               895|                895|               895|               895|                 895|                895|                895|\n",
      "|   mean|41.815642458100555|10102.777988826821|0.48044692737430167| 5.279318435754184| 8.588826815642458|                NULL|0.16759776536312848|  9.191290188135513|\n",
      "| stddev| 6.126060242946418|2352.6813239307294|0.49989688112158687|1.2734358235480647|1.7636296554821884|                NULL|0.37371754637596827|0.24851609510340106|\n",
      "|    min|              22.0|            4111.4|                  0|               1.0|               3.0|     Abbott-Thompson|                  0|  8.321518882091572|\n",
      "|    max|              65.0|          18026.01|                  1|              9.15|              14.0|Zuniga, Clark and...|                  1|  9.799570993870445|\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+--------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sub.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sub = df_sub.withColumn('Account_Manager', df_sub.Account_Manager.cast(\"string\"))\n",
    "# df_sub = df_sub.withColumn('Churn', df_sub.Churn.cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      " |-- log_Total_Purchase: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sub.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols= ['Age', 'log_Total_Purchase','Account_Manager', 'Years', 'Num_Sites'],\n",
    "                            outputCol = 'features')\n",
    "data_pre = assembler.transform(df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+---------------+-----+---------+-------------------+----------+-----+------------------+--------------------+\n",
      "| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|   Company|Churn|log_Total_Purchase|            features|\n",
      "+----+--------------+---------------+-----+---------+-------------------+----------+-----+------------------+--------------------+\n",
      "|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:40|Harvey LLC|    1| 9.311704914356662|[42.0,9.311704914...|\n",
      "|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:46|Wilson PLC|    1| 9.385655776234074|[41.0,9.385655776...|\n",
      "+----+--------------+---------------+-----+---------+-------------------+----------+-----+------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_pre.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-----+\n",
      "|features                              |Churn|\n",
      "+--------------------------------------+-----+\n",
      "|[42.0,9.311704914356662,0.0,7.22,8.0] |1    |\n",
      "|[41.0,9.385655776234074,0.0,6.5,11.0] |1    |\n",
      "|[38.0,9.463799720492338,0.0,6.67,12.0]|1    |\n",
      "+--------------------------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_pre.select('features', 'Churn').show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "895"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data = data_pre.select('features', 'Churn')\n",
    "final_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "895"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data = final_data.na.drop()\n",
    "final_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(\n",
    "    featuresCol = 'features', labelCol = 'Churn', predictionCol = 'prediction'\n",
    ")\n",
    "\n",
    "logisticModel = logistic.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|Churn|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|   22|\n",
      "|    0|       0.0|  218|\n",
      "|    1|       1.0|   27|\n",
      "|    0|       1.0|    7|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model = logisticModel.transform(test_data)\n",
    "test_model.groupBy('Churn', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.79\n",
      "recall    = 0.55\n",
      "accuracy = 0.89\n"
     ]
    }
   ],
   "source": [
    "# Calculate the elements of the confusion matrix\n",
    "TN = test_model.filter('prediction = 0 AND Churn = prediction').count()\n",
    "TP = test_model.filter('prediction = 1 AND Churn = prediction').count()\n",
    "FN = test_model.filter('prediction = 0 AND Churn != prediction').count()\n",
    "FP = test_model.filter('prediction = 1 AND Churn != prediction').count()\n",
    "# Calculate precision and recall\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print('precision = {:.2f}\\nrecall    = {:.2f}'.format(precision, recall))\n",
    "acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "print('accuracy = {:.2f}'.format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = test_model.withColumn('label', test_model.Churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8941605839416058"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_evaluator = MulticlassClassificationEvaluator()\n",
    "\n",
    "acc_ = multi_evaluator.evaluate(test_model, {multi_evaluator.metricName : 'accuracy'})\n",
    "acc_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9241723356009068"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_evaluator = BinaryClassificationEvaluator()\n",
    "auc = binary_evaluator.evaluate(test_model, {multi_evaluator.metricName : 'areaUnderROC'})\n",
    "auc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "logisticModel.save('logisticModel_customer_churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticModel2 = LogisticRegressionModel.load('logisticModel_customer_churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on new data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+\n",
      "|         Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|         Company|\n",
      "+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+\n",
      "| Andrew Mccall|37.0|       9935.53|              1| 7.71|      8.0|2011-08-29 18:37:54|38612 Johnny Stra...|        King Ltd|\n",
      "|Michele Wright|23.0|       7526.94|              1| 9.28|     15.0|2013-07-22 18:19:54|21083 Nicole Junc...|   Cannon-Benson|\n",
      "|  Jeremy Chang|65.0|         100.0|              1|  1.0|     15.0|2006-12-11 07:48:13|085 Austin Views ...|Barron-Robertson|\n",
      "+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data = spark.read.csv('new_customers.csv', header = True, inferSchema = True)\n",
    "new_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = new_data.withColumn('log_Total_Purchase', log(col('Total_Purchase')))\n",
    "new_data_pre = assembler.transform(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+------------------+--------------------+\n",
      "|         Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|         Company|log_Total_Purchase|            features|\n",
      "+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+------------------+--------------------+\n",
      "| Andrew Mccall|37.0|       9935.53|              1| 7.71|      8.0|2011-08-29 18:37:54|38612 Johnny Stra...|        King Ltd|  9.20387250031693|[37.0,9.203872500...|\n",
      "|Michele Wright|23.0|       7526.94|              1| 9.28|     15.0|2013-07-22 18:19:54|21083 Nicole Junc...|   Cannon-Benson| 8.926243863699453|[23.0,8.926243863...|\n",
      "|  Jeremy Chang|65.0|         100.0|              1|  1.0|     15.0|2006-12-11 07:48:13|085 Austin Views ...|Barron-Robertson| 4.605170185988092|[65.0,4.605170185...|\n",
      "+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data_pre.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-----------------------------------------+----------+\n",
      "|features                              |probability                              |prediction|\n",
      "+--------------------------------------+-----------------------------------------+----------+\n",
      "|[37.0,9.20387250031693,1.0,7.71,8.0]  |[0.9062513301918502,0.09374866980814978] |0.0       |\n",
      "|[23.0,8.926243863699453,1.0,9.28,15.0]|[0.003849241423576753,0.9961507585764232]|1.0       |\n",
      "|[65.0,4.605170185988092,1.0,1.0,15.0] |[0.01726223230510628,0.9827377676948937] |1.0       |\n",
      "|[32.0,8.777632527474145,0.0,9.4,14.0] |[0.009708056459548352,0.9902919435404517]|1.0       |\n",
      "|[32.0,9.484002877954307,1.0,10.0,8.0] |[0.8048534344953365,0.19514656550466347] |0.0       |\n",
      "+--------------------------------------+-----------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unlabeled_data = new_data_pre.select('features')\n",
    "predictions = logisticModel2.transform(unlabeled_data)\n",
    "predictions[['features', 'probability', 'prediction']].show(5,False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
